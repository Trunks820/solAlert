#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åˆå§‹åŒ– SOL WebSocket æ‰¹æ¬¡æ± 
ä» quick_monitor_templateã€twitter_account_manageã€token_launch_history è¯»å–æ•°æ®
è°ƒç”¨ DBotX API è·å– pair åœ°å€ï¼ŒæŒ‰å¸‚å€¼æ’åºå¹¶åˆ†æ‰¹ï¼Œå†™å…¥ sol_ws_batch_pool

ä½¿ç”¨æ–¹æ³•ï¼š
  python scripts/initialize_sol_ws_batches.py              # å®Œæ•´åˆå§‹åŒ–ï¼ˆæ¸…ç©ºé‡å»ºï¼‰
  python scripts/initialize_sol_ws_batches.py --incremental # å¢é‡æ¨¡å¼ï¼ˆåªå¤„ç†æ–°CAï¼‰
"""
import sys
import os
import asyncio
import json
import argparse
from typing import List, Dict, Set, Optional
from datetime import datetime
from decimal import Decimal

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

from src.solalert.core.database import DatabaseManager
from src.solalert.api.dbotx_api import DBotXAPI


class SolWsBatchInitializer:
    """SOL WebSocket æ‰¹æ¬¡æ± åˆå§‹åŒ–å™¨"""
    
    def __init__(self, incremental: bool = False):
        self.db = DatabaseManager()
        self.api = DBotXAPI()
        self.incremental = incremental
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_templates': 0,
            'total_profile_urls': 0,
            'total_matched_cas': 0,
            'existing_cas': 0,
            'new_cas_to_process': 0,
            'total_with_pairs': 0,
            'total_batches': 0,
            'failed_pairs': []
        }
    
    def load_templates(self) -> List[Dict]:
        """åŠ è½½æ¨¡æ¿é…ç½®"""
        print("=" * 80)
        print("æ­¥éª¤ 1/6: åŠ è½½æ¨¡æ¿é…ç½®")
        print("=" * 80)
        
        sql = """
            SELECT 
                id, config_name, min_market_cap, has_twitter,
                time_interval, top_holders_threshold, events_config, trigger_logic
            FROM quick_monitor_template
            WHERE del_flag = 0
            ORDER BY min_market_cap DESC
        """
        
        templates = self.db.execute_query(sql)
        self.stats['total_templates'] = len(templates)
        
        print(f"âœ… åŠ è½½äº† {len(templates)} ä¸ªæ¨¡æ¿é…ç½®ï¼š")
        for t in templates:
            print(f"   - ID={t['id']}, åç§°={t['config_name']}, "
                  f"æœ€å°å¸‚å€¼=${t['min_market_cap']:,.0f}, "
                  f"Twitterè¦æ±‚={t['has_twitter']}")
        
        return templates
    
    def get_profile_twitter_urls(self) -> Set[str]:
        """è·å–æ‰€æœ‰ profile ç±»å‹çš„ Twitter URLs"""
        print("\n" + "=" * 80)
        print("æ­¥éª¤ 2/6: è·å– Profile Twitter URLs")
        print("=" * 80)
        
        sql = """
            SELECT DISTINCT twitter_url
            FROM twitter_account_manage
            WHERE twitter_type = 'profile'
            AND twitter_url IS NOT NULL
            AND twitter_url != ''
        """
        
        rows = self.db.execute_query(sql)
        urls = {row['twitter_url'] for row in rows}
        self.stats['total_profile_urls'] = len(urls)
        
        print(f"âœ… è·å–äº† {len(urls):,} ä¸ª profile ç±»å‹çš„ Twitter URL")
        
        return urls
    
    def get_existing_cas(self) -> Set[str]:
        """è·å–æ•°æ®åº“ä¸­å·²æœ‰çš„CAåˆ—è¡¨ï¼ˆå¢é‡æ¨¡å¼ä½¿ç”¨ï¼‰"""
        if not self.incremental:
            return set()
        
        sql = """
            SELECT DISTINCT ca
            FROM sol_ws_batch_pool
            WHERE is_active = 1
        """
        
        rows = self.db.execute_query(sql)
        existing_cas = {row['ca'] for row in rows}
        self.stats['existing_cas'] = len(existing_cas)
        
        print(f"\nğŸ“¦ å¢é‡æ¨¡å¼: æ•°æ®åº“ä¸­å·²æœ‰ {len(existing_cas):,} ä¸ªCA")
        
        return existing_cas
    
    def match_cas_with_templates(
        self, 
        templates: List[Dict], 
        profile_urls: Set[str],
        existing_cas: Set[str] = None
    ) -> List[Dict]:
        """
        æŸ¥è¯¢å¹¶åŒ¹é…CAåˆ°å¯¹åº”æ¨¡æ¿
        
        Args:
            templates: æ¨¡æ¿é…ç½®åˆ—è¡¨
            profile_urls: profileç±»å‹Twitter URLé›†åˆ
            existing_cas: å·²å­˜åœ¨çš„CAé›†åˆï¼ˆå¢é‡æ¨¡å¼ä½¿ç”¨ï¼‰
        
        Returns:
            List of dict with keys: ca, token_symbol, token_name, market_cap, 
                                    twitter_url, template_id, template_name, 
                                    time_interval, events_config, trigger_logic
        """
        print("\n" + "=" * 80)
        print("æ­¥éª¤ 3/6: æŸ¥è¯¢å¹¶åŒ¹é… CA")
        print("=" * 80)
        
        if existing_cas is None:
            existing_cas = set()
        
        # æŒ‰å¸‚å€¼åŒºé—´åˆ’åˆ†æ¨¡æ¿ï¼ˆé™åºï¼š$10M, $1M, $500K, $300Kï¼‰
        templates_sorted = sorted(templates, key=lambda x: x['min_market_cap'], reverse=True)
        
        all_cas = []
        
        for i, template in enumerate(templates_sorted):
            min_cap = template['min_market_cap']
            
            # ç¡®å®šå¸‚å€¼åŒºé—´
            # ç¬¬ä¸€ä¸ªæ¨¡æ¿ï¼ˆæœ€é«˜å¸‚å€¼ï¼‰ï¼šå¸‚å€¼ >= min_capï¼ˆæ— ä¸Šé™ï¼‰
            # å…¶ä»–æ¨¡æ¿ï¼šå‰ä¸€ä¸ªæ¨¡æ¿çš„ min_cap <= å¸‚å€¼ < å½“å‰æ¨¡æ¿çš„ min_cap
            if i == 0:
                # ç¬¬ä¸€ä¸ªæ¨¡æ¿ï¼ˆå¦‚ $10Mï¼‰ï¼šå¸‚å€¼ >= $10M
                cap_condition = f"highest_market_cap >= {min_cap}"
            else:
                # åç»­æ¨¡æ¿ï¼šä¸Šä¸€ä¸ªæ¨¡æ¿çš„min_cap ä½œä¸ºå½“å‰çš„ä¸Šé™
                max_cap = templates_sorted[i - 1]['min_market_cap']
                cap_condition = f"highest_market_cap >= {min_cap} AND highest_market_cap < {max_cap}"
            
            # æŸ¥è¯¢CA
            sql = f"""
                SELECT 
                    ca,
                    token_symbol,
                    token_name,
                    highest_market_cap as market_cap,
                    twitter_url
                FROM token_launch_history
                WHERE source IN ('pump', 'bonk')
                AND {cap_condition}
                AND twitter_url IS NOT NULL
                AND twitter_url != ''
            """
            
            rows = self.db.execute_query(sql)
            
            # è¿‡æ»¤ï¼šåªä¿ç•™ twitter_url åœ¨ profile_urls ä¸­çš„
            matched_rows = [
                row for row in rows 
                if row['twitter_url'] in profile_urls
            ]
            
            # å¢é‡æ¨¡å¼ï¼šè¿‡æ»¤æ‰å·²å­˜åœ¨çš„CA
            if self.incremental and existing_cas:
                new_matched_rows = [
                    row for row in matched_rows
                    if row['ca'] not in existing_cas
                ]
                skipped = len(matched_rows) - len(new_matched_rows)
                matched_rows = new_matched_rows
            else:
                skipped = 0
            
            print(f"\nğŸ“Š æ¨¡æ¿ [{template['config_name']}] (ID={template['id']})")
            if i == 0:
                print(f"   å¸‚å€¼åŒºé—´: ${min_cap:,.0f} ä»¥ä¸Š")
            else:
                max_cap = templates_sorted[i - 1]['min_market_cap']
                print(f"   å¸‚å€¼åŒºé—´: ${min_cap:,.0f} - ${max_cap:,.0f}")
            print(f"   æŸ¥è¯¢åˆ°: {len(rows):,} ä¸ªCA")
            if self.incremental and existing_cas:
                print(f"   åŒ¹é… profile: {len(matched_rows) + skipped:,} ä¸ªCA (è·³è¿‡å·²æœ‰: {skipped})")
                print(f"   æ–°å¢å¤„ç†: {len(matched_rows):,} ä¸ªCA")
            else:
                print(f"   åŒ¹é… profile: {len(matched_rows):,} ä¸ªCA")
            
            # æ·»åŠ æ¨¡æ¿ä¿¡æ¯
            for row in matched_rows:
                all_cas.append({
                    'ca': row['ca'],
                    'token_symbol': row['token_symbol'],
                    'token_name': row['token_name'],
                    'market_cap': float(row['market_cap']) if row['market_cap'] else 0.0,
                    'twitter_url': row['twitter_url'],
                    'template_id': template['id'],
                    'template_name': template['config_name'],
                    'time_interval': template['time_interval'],
                    'events_config': template['events_config'],
                    'trigger_logic': template['trigger_logic']
                })
        
        self.stats['total_matched_cas'] = len(all_cas)
        self.stats['new_cas_to_process'] = len(all_cas)
        
        if self.incremental and existing_cas:
            print(f"\nâœ… å¢é‡æ¨¡å¼æ€»è®¡:")
            print(f"   å·²æœ‰CA: {len(existing_cas):,}")
            print(f"   æ–°å¢CA: {len(all_cas):,}")
        else:
            print(f"\nâœ… æ€»è®¡åŒ¹é…åˆ° {len(all_cas):,} ä¸ªCA")
        
        return all_cas
    
    async def fetch_pair_addresses(self, cas: List[Dict]) -> List[Dict]:
        """
        å¹¶å‘è·å–æ¯ä¸ªCAçš„ pair åœ°å€
        ä½¿ç”¨é™æµæ§åˆ¶é¿å…APIé™æµ
        """
        print("\n" + "=" * 80)
        print("æ­¥éª¤ 4/6: è·å– Pair åœ°å€")
        print("=" * 80)
        
        print(f"ğŸ” å¼€å§‹æŸ¥è¯¢ {len(cas):,} ä¸ªCAçš„pairåœ°å€...")
        print(f"   ç­–ç•¥: 10ä¸ª/æ‰¹ï¼Œä¸²è¡Œæ‰¹æ¬¡ï¼Œæ¯æ‰¹é—´éš”3ç§’ï¼ˆé¿å…APIé™æµï¼‰")
        
        results = []
        batch_size = 50  # é™ä½åˆ°10ä¸ª/æ‰¹
        
        for i in range(0, len(cas), batch_size):
            batch = cas[i:i+batch_size]
            batch_num = i // batch_size + 1
            total_batches = (len(cas) + batch_size - 1) // batch_size
            
            print(f"\n   æ‰¹æ¬¡ {batch_num}/{total_batches}: å¤„ç† {len(batch)} ä¸ªCA")
            
            # å¹¶å‘æŸ¥è¯¢ï¼ˆä½†æ‰¹æ¬¡æ•°é‡å°ï¼‰
            tasks = [
                self._fetch_single_pair(ca_info)
                for ca_info in batch
            ]
            
            batch_results = await asyncio.gather(*tasks)
            
            # ç»Ÿè®¡æˆåŠŸ/å¤±è´¥
            success_count = sum(1 for r in batch_results if r is not None)
            failed_count = len(batch_results) - success_count
            
            print(f"      æˆåŠŸ: {success_count}, å¤±è´¥: {failed_count}")
            
            # æ·»åŠ æˆåŠŸçš„ç»“æœ
            for result in batch_results:
                if result is not None:
                    results.append(result)
            
            # æ‰¹æ¬¡é—´å»¶è¿Ÿï¼ˆå¢åŠ åˆ°3ç§’ï¼‰
            if i + batch_size < len(cas):
                await asyncio.sleep(3.0)
                print(f"      â³ ç­‰å¾…3ç§’åç»§ç»­...")
        
        self.stats['total_with_pairs'] = len(results)
        
        print(f"\nâœ… æˆåŠŸè·å– {len(results):,} ä¸ªCAçš„pairåœ°å€")
        if self.stats['failed_pairs']:
            print(f"âŒ å¤±è´¥ {len(self.stats['failed_pairs'])} ä¸ªCA")
            print(f"   å¤±è´¥çš„CAå·²è®°å½•ï¼Œåç»­å¯æ‰‹åŠ¨è¡¥å……")
        
        return results
    
    async def _fetch_single_pair(self, ca_info: Dict) -> Optional[Dict]:
        """è·å–å•ä¸ªCAçš„pairåœ°å€"""
        try:
            # è°ƒç”¨ DBotX APIï¼ˆåªéœ€è¦ä¼ CAåœ°å€ï¼‰
            pair_data = await self.api.search_pairs(ca_info['ca'])
            
            if not pair_data:
                raise ValueError(f"æœªæ‰¾åˆ°pairæ•°æ®")
            
            # search_pairs è¿”å›çš„æ˜¯å•ä¸ªå­—å…¸ï¼ŒåŒ…å« pair_address é”®
            pair_address = pair_data.get('pair_address')
            
            if not pair_address:
                raise ValueError(f"pairåœ°å€ä¸ºç©º")
            
            # è¿”å›å®Œæ•´ä¿¡æ¯
            return {
                **ca_info,
                'pair_address': pair_address
            }
        
        except Exception as e:
            # è®°å½•å¤±è´¥
            self.stats['failed_pairs'].append({
                'ca': ca_info['ca'],
                'token_symbol': ca_info.get('token_symbol'),
                'error': str(e)
            })
            return None
    
    def sort_and_batch(self, cas_with_pairs: List[Dict]) -> List[List[Dict]]:
        """
        æŒ‰å¸‚å€¼æ’åºå¹¶åˆ†æ‰¹
        æ¯æ‰¹æœ€å¤š99ä¸ªCA
        """
        print("\n" + "=" * 80)
        print("æ­¥éª¤ 5/6: æ’åºå¹¶åˆ†æ‰¹")
        print("=" * 80)
        
        # æŒ‰å¸‚å€¼é™åºæ’åº
        sorted_cas = sorted(
            cas_with_pairs,
            key=lambda x: x['market_cap'],
            reverse=True
        )
        
        # åˆ†æ‰¹ï¼ˆæ¯æ‰¹99ä¸ªï¼‰
        batch_size = 99
        batches = []
        
        for i in range(0, len(sorted_cas), batch_size):
            batch = sorted_cas[i:i+batch_size]
            batches.append(batch)
        
        self.stats['total_batches'] = len(batches)
        
        print(f"âœ… å…±åˆ†ä¸º {len(batches)} ä¸ªæ‰¹æ¬¡ï¼š")
        for i, batch in enumerate(batches, 1):
            print(f"   æ‰¹æ¬¡ {i}: {len(batch)} ä¸ªCA")
        
        return batches
    
    def write_to_database(self, batches: List[List[Dict]]):
        """
        å†™å…¥æ‰¹æ¬¡æ± è¡¨å’Œæ‰¹æ¬¡ç»Ÿè®¡è¡¨
        """
        print("\n" + "=" * 80)
        print("æ­¥éª¤ 6/6: å†™å…¥æ•°æ®åº“")
        print("=" * 80)
        
        # 1. æ¸…ç©ºç°æœ‰æ•°æ®ï¼ˆä»…åœ¨éå¢é‡æ¨¡å¼ï¼‰
        if not self.incremental:
            print("ğŸ—‘ï¸  æ¸…ç©ºç°æœ‰æ‰¹æ¬¡æ± æ•°æ®...")
            self.db.execute_update("DELETE FROM sol_ws_batch_pool")
            self.db.execute_update("DELETE FROM sol_ws_batch_stats")
        else:
            print("ğŸ“¦ å¢é‡æ¨¡å¼: ä¿ç•™ç°æœ‰æ•°æ®ï¼Œè¿½åŠ æ–°æ•°æ®...")
        
        # 2. å†™å…¥æ‰¹æ¬¡æ± 
        print(f"ğŸ’¾ å†™å…¥ {len(batches)} ä¸ªæ‰¹æ¬¡åˆ° sol_ws_batch_pool...")
        
        # æŸ¥è¯¢å½“å‰æœ€å¤§çš„ batch_id å’Œ priorityï¼ˆå¢é‡æ¨¡å¼ï¼‰
        if self.incremental:
            max_info = self.db.execute_query("""
                SELECT 
                    COALESCE(MAX(batch_id), 0) as max_batch_id,
                    COALESCE(MAX(priority), 0) as max_priority
                FROM sol_ws_batch_pool
            """, fetch_one=True)
            
            start_batch_id = max_info['max_batch_id'] + 1
            start_priority = max_info['max_priority'] + 1
            print(f"   ä»æ‰¹æ¬¡ {start_batch_id} å¼€å§‹ï¼Œä¼˜å…ˆçº§ä» {start_priority} å¼€å§‹")
        else:
            start_batch_id = 1
            start_priority = len(batches) * 99  # ä»æœ€å¤§å€¼å¼€å§‹é€’å‡
        
        insert_sql = """
            INSERT INTO sol_ws_batch_pool (
                batch_id, ca, token_symbol, token_name, pair_address,
                market_cap, twitter_url, template_id, template_name,
                time_interval, events_config, trigger_logic,
                priority, sort_order, is_active
            ) VALUES (
                %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s
            )
        """
        
        total_inserted = 0
        global_priority = start_priority
        
        for i, batch in enumerate(batches):
            batch_id = start_batch_id + i
            print(f"\n   æ‰¹æ¬¡ {batch_id}: {len(batch)} ä¸ªCA")
            
            for sort_order, ca_info in enumerate(batch):
                self.db.execute_update(insert_sql, (
                    batch_id,
                    ca_info['ca'],
                    ca_info['token_symbol'],
                    ca_info['token_name'],
                    ca_info['pair_address'],
                    ca_info['market_cap'],
                    ca_info['twitter_url'],
                    ca_info['template_id'],
                    ca_info['template_name'],
                    ca_info['time_interval'],
                    ca_info['events_config'],
                    ca_info['trigger_logic'],
                    global_priority,
                    sort_order,
                    1  # is_active
                ))
                
                total_inserted += 1
                # å¢é‡æ¨¡å¼ï¼špriorityé€’å¢ï¼›éå¢é‡æ¨¡å¼ï¼špriorityé€’å‡
                if self.incremental:
                    global_priority += 1
                else:
                    global_priority -= 1
            
            print(f"      âœ“ å·²æ’å…¥ {len(batch)} æ¡è®°å½•")
        
        # 3. åˆå§‹åŒ–æ‰¹æ¬¡ç»Ÿè®¡è¡¨
        print(f"\nğŸ“Š åˆå§‹åŒ–æ‰¹æ¬¡ç»Ÿè®¡è¡¨...")
        
        if self.incremental:
            # å¢é‡æ¨¡å¼ï¼šæ’å…¥æ–°æ‰¹æ¬¡æˆ–æ›´æ–°å·²æœ‰æ‰¹æ¬¡
            stats_sql = """
                INSERT INTO sol_ws_batch_stats (
                    batch_id, ca_count, subscribed_count, alert_count, 
                    total_messages, error_count
                ) VALUES (
                    %s, %s, 0, 0, 0, 0
                )
                ON DUPLICATE KEY UPDATE
                    ca_count = ca_count + VALUES(ca_count)
            """
        else:
            # éå¢é‡æ¨¡å¼ï¼šç›´æ¥æ’å…¥
            stats_sql = """
                INSERT INTO sol_ws_batch_stats (
                    batch_id, ca_count, subscribed_count, alert_count, 
                    total_messages, error_count
                ) VALUES (
                    %s, %s, 0, 0, 0, 0
                )
            """
        
        for i, batch in enumerate(batches):
            batch_id = start_batch_id + i
            self.db.execute_update(stats_sql, (batch_id, len(batch)))
        
        print(f"   âœ“ å·²åˆå§‹åŒ– {len(batches)} ä¸ªæ‰¹æ¬¡ç»Ÿè®¡")
        
        print(f"\nâœ… æ•°æ®å†™å…¥å®Œæˆï¼")
        print(f"   æ€»è®¡æ’å…¥: {total_inserted} æ¡è®°å½•")
    
    def save_failed_pairs(self):
        """ä¿å­˜å¤±è´¥çš„CAåˆ—è¡¨åˆ°æ–‡ä»¶"""
        if not self.stats['failed_pairs']:
            return
        
        failed_file = os.path.join(project_root, 'logs', 'failed_pairs.json')
        
        try:
            # ç¡®ä¿logsç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(failed_file), exist_ok=True)
            
            # ä¿å­˜å¤±è´¥ä¿¡æ¯
            with open(failed_file, 'w', encoding='utf-8') as f:
                json.dump({
                    'timestamp': datetime.now().isoformat(),
                    'total_failed': len(self.stats['failed_pairs']),
                    'failed_pairs': self.stats['failed_pairs']
                }, f, indent=2, ensure_ascii=False)
            
            print(f"\nğŸ’¾ å¤±è´¥çš„CAåˆ—è¡¨å·²ä¿å­˜åˆ°: {failed_file}")
            print(f"   å¯ä½¿ç”¨è¡¥å……è„šæœ¬é‡è¯•: python scripts/retry_failed_pairs.py")
        
        except Exception as e:
            print(f"\nâš ï¸  ä¿å­˜å¤±è´¥CAåˆ—è¡¨æ—¶å‡ºé”™: {e}")
    
    def print_summary(self):
        """æ‰“å°æ€»ç»“æŠ¥å‘Š"""
        print("\n" + "=" * 80)
        print("åˆå§‹åŒ–å®Œæˆ - æ€»ç»“æŠ¥å‘Š")
        print("=" * 80)
        
        print(f"\nğŸ“Š æ•°æ®ç»Ÿè®¡:")
        print(f"   æ¨¡æ¿é…ç½®æ•°é‡:      {self.stats['total_templates']}")
        print(f"   Profile URLs:      {self.stats['total_profile_urls']:,}")
        
        if self.incremental:
            print(f"   å·²æœ‰CAæ•°é‡:        {self.stats['existing_cas']:,}")
            print(f"   æ–°å¢CAæ•°é‡:        {self.stats['new_cas_to_process']:,}")
            print(f"   è·å–åˆ°Pairçš„CA:    {self.stats['total_with_pairs']:,}")
        else:
            print(f"   åŒ¹é…çš„CAæ•°é‡:      {self.stats['total_matched_cas']:,}")
            print(f"   è·å–åˆ°Pairçš„CA:    {self.stats['total_with_pairs']:,}")
        
        print(f"   å¤±è´¥çš„CAæ•°é‡:      {len(self.stats['failed_pairs'])}")
        print(f"   ç”Ÿæˆçš„æ‰¹æ¬¡æ•°:      {self.stats['total_batches']}")
        
        if self.stats['failed_pairs']:
            print(f"\nâŒ å¤±è´¥çš„CAåˆ—è¡¨ï¼ˆå‰10ä¸ªï¼‰:")
            for fail in self.stats['failed_pairs'][:10]:
                print(f"   - {fail['ca']} ({fail['token_symbol']}): {fail['error']}")
            
            if len(self.stats['failed_pairs']) > 10:
                print(f"   ... è¿˜æœ‰ {len(self.stats['failed_pairs']) - 10} ä¸ª")
        
        print(f"\nâœ… æ‰¹æ¬¡æ± å·²å°±ç»ªï¼")
        print(f"   å¯ä»¥å¯åŠ¨ç›‘æ§ç¨‹åºï¼špython -m src.solalert.monitor.sol_websocket_manager")
    
    async def run(self):
        """è¿è¡Œåˆå§‹åŒ–æµç¨‹"""
        print("\n")
        print("â•”" + "=" * 78 + "â•—")
        mode_text = "å¢é‡æ›´æ–°" if self.incremental else "å®Œæ•´åˆå§‹åŒ–"
        title = f"SOL WebSocket æ‰¹æ¬¡æ± åˆå§‹åŒ–å·¥å…· ({mode_text})"
        padding = (78 - len(title.encode('gbk'))) // 2
        print("â•‘" + " " * padding + title + " " * (78 - padding - len(title.encode('gbk'))) + "â•‘")
        print("â•š" + "=" * 78 + "â•")
        print(f"\nå¯åŠ¨æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"è¿è¡Œæ¨¡å¼: {'å¢é‡æ¨¡å¼ï¼ˆåªå¤„ç†æ–°CAï¼‰' if self.incremental else 'å®Œæ•´æ¨¡å¼ï¼ˆæ¸…ç©ºé‡å»ºï¼‰'}\n")
        
        try:
            # 1. åŠ è½½æ¨¡æ¿é…ç½®
            templates = self.load_templates()
            
            # 2. è·å– profile Twitter URLs
            profile_urls = self.get_profile_twitter_urls()
            
            # 2.5. è·å–å·²æœ‰CAåˆ—è¡¨ï¼ˆå¢é‡æ¨¡å¼ï¼‰
            existing_cas = self.get_existing_cas()
            
            # 3. æŸ¥è¯¢å¹¶åŒ¹é…CA
            all_cas = self.match_cas_with_templates(templates, profile_urls, existing_cas)
            
            if not all_cas:
                print("\nâŒ æ²¡æœ‰åŒ¹é…åˆ°ä»»ä½•CAï¼Œé€€å‡ºåˆå§‹åŒ–")
                return
            
            # 4. è·å– pair åœ°å€
            cas_with_pairs = await self.fetch_pair_addresses(all_cas)
            
            if not cas_with_pairs:
                print("\nâŒ æ²¡æœ‰è·å–åˆ°ä»»ä½•pairåœ°å€ï¼Œé€€å‡ºåˆå§‹åŒ–")
                return
            
            # 5. æ’åºå¹¶åˆ†æ‰¹
            batches = self.sort_and_batch(cas_with_pairs)
            
            # 6. å†™å…¥æ•°æ®åº“
            self.write_to_database(batches)
            
            # 7. ä¿å­˜å¤±è´¥çš„CAåˆ—è¡¨
            self.save_failed_pairs()
            
            # 8. æ‰“å°æ€»ç»“
            self.print_summary()
        
        except KeyboardInterrupt:
            print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­åˆå§‹åŒ–")
        except Exception as e:
            print(f"\n\nâŒ åˆå§‹åŒ–å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()


async def main():
    """ä¸»å‡½æ•°"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(
        description='SOL WebSocket æ‰¹æ¬¡æ± åˆå§‹åŒ–å·¥å…·',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog='''
ä½¿ç”¨ç¤ºä¾‹:
  å®Œæ•´åˆå§‹åŒ–ï¼ˆæ¸…ç©ºé‡å»ºï¼‰:
    python scripts/initialize_sol_ws_batches.py
  
  å¢é‡æ¨¡å¼ï¼ˆåªå¤„ç†æ–°CAï¼‰:
    python scripts/initialize_sol_ws_batches.py --incremental
        '''
    )
    
    parser.add_argument(
        '--incremental',
        action='store_true',
        help='å¢é‡æ¨¡å¼ï¼šåªå¤„ç†æ–°CAï¼Œä¸æ¸…ç©ºç°æœ‰æ•°æ®'
    )
    
    args = parser.parse_args()
    
    # åˆ›å»ºåˆå§‹åŒ–å™¨å¹¶è¿è¡Œ
    initializer = SolWsBatchInitializer(incremental=args.incremental)
    await initializer.run()


if __name__ == "__main__":
    asyncio.run(main())

